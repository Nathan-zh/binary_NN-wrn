{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../mixnet_k/wide_resnet_5/poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./datagen.py\n",
    "datagen, (x_train, y_train), (x_test, y_test) = data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = '../mixnet_k/wide_resnet_5/'\n",
    "batch_size = 100\n",
    "iterations = x_train.shape[0] // batch_size\n",
    "epochs = 500\n",
    "old_acc = 0\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=1e-4)\n",
    "\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(x, i):\n",
    "    \n",
    "    # ax^2 + bx + c\n",
    "        \n",
    "    if i == 1: \n",
    "        first_coef = tf.Variable(0.31486639, trainable=True, name='first_coef', dtype=tf.float32)\n",
    "        second_coef = tf.Variable(0.49538696, trainable=True, name='second_coef', dtype=tf.float32)\n",
    "        third_coef = tf.Variable(0.1256594, trainable=True, name='third_coef', dtype=tf.float32)\n",
    "        y = tf.math.polyval([first_coef, second_coef, third_coef], x)\n",
    "        \n",
    "    elif i == 2:  \n",
    "        first_coef = tf.Variable(0.14825453, trainable=True, name='first_coef', dtype=tf.float32)\n",
    "        second_coef = tf.Variable(0.49779217, trainable=True, name='second_coef', dtype=tf.float32)\n",
    "        third_coef = tf.Variable(0.26774763, trainable=True, name='third_coef', dtype=tf.float32)\n",
    "        y = tf.math.polyval([first_coef, second_coef, third_coef], x)\n",
    "        \n",
    "    elif i == 3:  \n",
    "        first_coef = tf.Variable(0.15073162, trainable=True, name='first_coef', dtype=tf.float32)\n",
    "        second_coef = tf.Variable(0.48908482, trainable=True, name='second_coef', dtype=tf.float32)\n",
    "        third_coef = tf.Variable(0.25940762, trainable=True, name='third_coef', dtype=tf.float32)\n",
    "        y = tf.math.polyval([first_coef, second_coef, third_coef], x)\n",
    "\n",
    "    elif i == 4:  \n",
    "        first_coef = tf.Variable(0.14494386, trainable=True, name='first_coef', dtype=tf.float32)\n",
    "        second_coef = tf.Variable(0.48223472, trainable=True, name='second_coef', dtype=tf.float32)\n",
    "        third_coef = tf.Variable(0.26587296, trainable=True, name='third_coef', dtype=tf.float32)\n",
    "        y = tf.math.polyval([first_coef, second_coef, third_coef], x)\n",
    "\n",
    "    elif i == 5:  \n",
    "        first_coef = tf.Variable(0.11254344, trainable=True, name='first_coef', dtype=tf.float32)\n",
    "        second_coef = tf.Variable(0.47676477, trainable=True, name='second_coef', dtype=tf.float32)\n",
    "        third_coef = tf.Variable(0.33750685, trainable=True, name='third_coef', dtype=tf.float32)\n",
    "        y = tf.math.polyval([first_coef, second_coef, third_coef], x)\n",
    "\n",
    "    else:  \n",
    "        first_coef = tf.Variable(0.08870094, trainable=True, name='first_coef', dtype=tf.float32)\n",
    "        second_coef = tf.Variable(0.49866869, trainable=True, name='second_coef', dtype=tf.float32)\n",
    "        third_coef = tf.Variable(0.44814718, trainable=True, name='third_coef', dtype=tf.float32)\n",
    "        y = tf.math.polyval([first_coef, second_coef, third_coef], x)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet layer\n",
    "def res_layer(inputs, filter_num, filter_size, stride, is_train, act_num, \n",
    "              batch_norm=True, activation=True):\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    if batch_norm:\n",
    "        x = tf.layers.batch_normalization(x, training=is_train)\n",
    "    if activation:\n",
    "        x = act(x, act_num)\n",
    "    x = tf.layers.conv2d(inputs=x, filters=filter_num, kernel_regularizer=regularizer, \n",
    "                         kernel_size=filter_size, strides=stride, padding='same')\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_resnet(inputs, k, is_train):\n",
    "    \n",
    "    act_num = 0\n",
    "\n",
    "    with tf.variable_scope(\"1st_Conv\"):\n",
    "        x = tf.layers.conv2d(inputs=inputs, filters=16, kernel_regularizer=regularizer, \n",
    "                             kernel_size=3, strides=1, padding='same')\n",
    "        x = tf.layers.batch_normalization(x, training=is_train)\n",
    "        x = tf.nn.relu(x)\n",
    "    \n",
    "    x_temp_0 = x\n",
    "    \n",
    "    with tf.variable_scope('ResBlock_%d_%d' % (1, 1)):\n",
    "        \n",
    "        with tf.variable_scope('conv1'):\n",
    "            x = res_layer(x, 16*k, 3, 1, is_train, act_num, \n",
    "                          batch_norm=False, activation=False)\n",
    "            act_num += 1\n",
    "            \n",
    "        x = tf.layers.dropout(x, 0.1)\n",
    "        \n",
    "        with tf.variable_scope('conv2'):\n",
    "            x = res_layer(x, 16*k, 3, 1, is_train, act_num)\n",
    "            act_num += 1\n",
    "    \n",
    "        with tf.variable_scope('x_plus_shortcut'):\n",
    "            shortcut = tf.layers.conv2d(inputs=x_temp_0, filters=16*k, \n",
    "                                        kernel_size=1, strides=1, padding='same')\n",
    "            x = x + shortcut             \n",
    "        \n",
    "    x_temp_1 = x\n",
    "    \n",
    "    with tf.variable_scope('ResBlock_%d_%d' % (2, 1)):\n",
    "        \n",
    "        with tf.variable_scope('conv1'):\n",
    "            x = res_layer(x, 32*k, 3, 2, is_train, act_num)\n",
    "        act_num += 1\n",
    "        \n",
    "        x = tf.layers.dropout(x, 0.1)\n",
    "                \n",
    "        with tf.variable_scope('conv2'):\n",
    "            x = res_layer(x, 32*k, 3, 1, is_train, act_num)\n",
    "        act_num += 1\n",
    "        \n",
    "        with tf.variable_scope('x_plus_shortcut'):\n",
    "            shortcut = tf.layers.conv2d(inputs=x_temp_0, filters=32*k, \n",
    "                                        kernel_size=1, strides=2, padding='same')\n",
    "            x = x + shortcut\n",
    "            \n",
    "            shortcut = tf.layers.conv2d(inputs=x_temp_1, filters=32*k, \n",
    "                                        kernel_size=1, strides=2, padding='same')\n",
    "            x = x + shortcut\n",
    "        \n",
    "    x_temp_2 = x    \n",
    "    \n",
    "    with tf.variable_scope('ResBlock_%d_%d' % (3, 1)):\n",
    "        \n",
    "        with tf.variable_scope('conv1'):\n",
    "            x = res_layer(x, 64*k, 3, 2, is_train, act_num)\n",
    "            act_num += 1\n",
    "            \n",
    "        x = tf.layers.dropout(x, 0.1)\n",
    "                \n",
    "        with tf.variable_scope('conv2'):\n",
    "            x = res_layer(x, 64*k, 3, 1, is_train, act_num)\n",
    "            act_num += 1\n",
    "            \n",
    "        with tf.variable_scope('x_plus_shortcut'):\n",
    "            \n",
    "            shortcut = tf.layers.conv2d(inputs=x_temp_0, filters=64*k, \n",
    "                                        kernel_size=1, strides=4, padding='same')\n",
    "            x = x + shortcut\n",
    "\n",
    "            shortcut = tf.layers.conv2d(inputs=x_temp_1, filters=64*k, \n",
    "                                        kernel_size=1, strides=4, padding='same')\n",
    "            x = x + shortcut\n",
    "\n",
    "            shortcut = tf.layers.conv2d(inputs=x_temp_2, filters=64*k, \n",
    "                                        kernel_size=1, strides=2, padding='same')\n",
    "            x = x + shortcut\n",
    "\n",
    "    with tf.variable_scope(\"AfterResBlock\"):\n",
    "        x = tf.layers.batch_normalization(x, training=is_train)\n",
    "        x = act(x, act_num)\n",
    "        x = tf.layers.average_pooling2d(x, pool_size=8, strides=8, padding='SAME', name='ave_pool')\n",
    "\n",
    "    with tf.variable_scope(\"Flatten\"):\n",
    "        x = tf.transpose(x, perm=[0, 3, 1, 2])\n",
    "        x = tf.layers.flatten(x)\n",
    "\n",
    "    with tf.variable_scope(\"Prediction\"):\n",
    "        pred = tf.layers.dense(x, units=10, kernel_regularizer=regularizer)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-db3249415e9d>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-db3249415e9d>:8: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-db3249415e9d>:20: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-db3249415e9d>:85: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.average_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-db3249415e9d>:89: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-db3249415e9d>:92: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/GPU:3'):\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32, [None, 32, 32, 3], name='input')\n",
    "    outputs = tf.placeholder(tf.float32, [None, 10], name='output')\n",
    "    is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    boundaries = [5*iterations, 10*iterations, 15*iterations]\n",
    "    values = [1e-3, 5e-4, 1e-4, 1e-5]\n",
    "    l_r = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "\n",
    "    tf.summary.scalar('learning_rate', l_r)\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=l_r)\n",
    "\n",
    "    pred = wide_resnet(inputs, k, is_train)\n",
    "    \n",
    "#     l2_loss = tf.losses.get_regularization_loss()\n",
    "    loss = tf.losses.softmax_cross_entropy(outputs, pred)\n",
    "    \n",
    "    grads = opt.compute_gradients(loss)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(outputs, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    restore_vars = []\n",
    "\n",
    "    for var in tf.global_variables()[1:51+18]:\n",
    "        if '_coef:0' not in var.name:\n",
    "            restore_vars.append(var)\n",
    "    saver = tf.train.Saver(restore_vars)\n",
    "    \n",
    "    new_saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "\n",
    "#     def add_hist(train_vars):\n",
    "#         for i in train_vars:\n",
    "#             name = i.name.split(\":\")[0] + '/value'\n",
    "#             value = i.value()\n",
    "#             tf.summary.histogram(name, value)\n",
    "\n",
    "#     add_hist(tf.trainable_variables())\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************2nd training stage*****************\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../mixnet_k/wide_resnet_5/cifar10.ckpt-183500\n",
      "Epoch: 1 Train_loss: 6.533 Val_acc: 0.346 Time consumed: 46.9670 s\n",
      "Epoch: 2 Train_loss: 2.015 Val_acc: 0.629 Time consumed: 42.5119 s\n",
      "Epoch: 3 Train_loss: 1.636 Val_acc: 0.732 Time consumed: 42.7098 s\n",
      "Epoch: 4 Train_loss: 1.168 Val_acc: 0.772 Time consumed: 42.7662 s\n",
      "Epoch: 5 Train_loss: 1.179 Val_acc: 0.790 Time consumed: 42.7067 s\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch: 6 Train_loss: 0.974 Val_acc: 0.810 Time consumed: 42.2202 s\n",
      "Epoch: 7 Train_loss: 0.922 Val_acc: 0.829 Time consumed: 42.2050 s\n",
      "Epoch: 8 Train_loss: 1.002 Val_acc: 0.836 Time consumed: 42.3981 s\n",
      "Epoch: 9 Train_loss: 0.735 Val_acc: 0.854 Time consumed: 42.6077 s\n",
      "Epoch: 10 Train_loss: 0.753 Val_acc: 0.820 Time consumed: 41.9642 s\n",
      "Epoch: 11 Train_loss: 0.724 Val_acc: 0.882 Time consumed: 42.2437 s\n",
      "Epoch: 12 Train_loss: 0.593 Val_acc: 0.886 Time consumed: 42.4062 s\n",
      "Epoch: 13 Train_loss: 0.562 Val_acc: 0.889 Time consumed: 42.6742 s\n",
      "Epoch: 14 Train_loss: 0.602 Val_acc: 0.881 Time consumed: 43.0837 s\n",
      "Epoch: 15 Train_loss: 0.546 Val_acc: 0.878 Time consumed: 42.0197 s\n",
      "Epoch: 16 Train_loss: 0.531 Val_acc: 0.887 Time consumed: 41.9883 s\n",
      "Epoch: 17 Train_loss: 0.463 Val_acc: 0.888 Time consumed: 41.8859 s\n",
      "Epoch: 18 Train_loss: 0.532 Val_acc: 0.889 Time consumed: 41.8775 s\n",
      "Epoch: 19 Train_loss: 0.491 Val_acc: 0.887 Time consumed: 41.7470 s\n",
      "Epoch: 20 Train_loss: 0.468 Val_acc: 0.888 Time consumed: 41.9281 s\n",
      "Epoch: 21 Train_loss: 0.448 Val_acc: 0.891 Time consumed: 42.4528 s\n",
      "Epoch: 22 Train_loss: 0.466 Val_acc: 0.889 Time consumed: 42.0574 s\n",
      "Epoch: 23 Train_loss: 0.565 Val_acc: 0.890 Time consumed: 42.0682 s\n",
      "Epoch: 24 Train_loss: 0.435 Val_acc: 0.894 Time consumed: 42.6145 s\n",
      "Epoch: 25 Train_loss: 0.434 Val_acc: 0.891 Time consumed: 41.9220 s\n",
      "Epoch: 248 Train_loss: 0.168 Val_acc: 0.890 Time consumed: 42.3237 s\n",
      "Epoch: 249 Train_loss: 0.171 Val_acc: 0.891 Time consumed: 42.0936 s\n",
      "Epoch: 250 Train_loss: 0.178 Val_acc: 0.893 Time consumed: 42.0623 s\n",
      "Epoch: 251 Train_loss: 0.167 Val_acc: 0.892 Time consumed: 42.0651 s\n",
      "Epoch: 252 Train_loss: 0.173 Val_acc: 0.892 Time consumed: 42.1550 s\n",
      "Epoch: 253 Train_loss: 0.165 Val_acc: 0.889 Time consumed: 41.9707 s\n",
      "Epoch: 254 Train_loss: 0.166 Val_acc: 0.891 Time consumed: 41.9265 s\n",
      "Epoch: 255 Train_loss: 0.169 Val_acc: 0.888 Time consumed: 42.1124 s\n",
      "Epoch: 256 Train_loss: 0.165 Val_acc: 0.893 Time consumed: 42.3707 s\n",
      "Epoch: 257 Train_loss: 0.165 Val_acc: 0.890 Time consumed: 42.2501 s\n",
      "Epoch: 258 Train_loss: 0.166 Val_acc: 0.892 Time consumed: 42.2780 s\n",
      "Epoch: 259 Train_loss: 0.170 Val_acc: 0.891 Time consumed: 41.7970 s\n",
      "Epoch: 260 Train_loss: 0.185 Val_acc: 0.892 Time consumed: 42.2133 s\n",
      "Epoch: 261 Train_loss: 0.164 Val_acc: 0.893 Time consumed: 42.2317 s\n",
      "Epoch: 262 Train_loss: 0.173 Val_acc: 0.892 Time consumed: 42.1664 s\n",
      "Epoch: 263 Train_loss: 0.178 Val_acc: 0.891 Time consumed: 42.0257 s\n",
      "Epoch: 264 Train_loss: 0.166 Val_acc: 0.891 Time consumed: 42.2676 s\n",
      "Epoch: 265 Train_loss: 0.162 Val_acc: 0.893 Time consumed: 41.6920 s\n",
      "Epoch: 266 Train_loss: 0.183 Val_acc: 0.892 Time consumed: 42.0593 s\n",
      "Epoch: 267 Train_loss: 0.163 Val_acc: 0.893 Time consumed: 42.0245 s\n",
      "Epoch: 268 Train_loss: 0.160 Val_acc: 0.891 Time consumed: 42.0987 s\n",
      "Epoch: 269 Train_loss: 0.160 Val_acc: 0.888 Time consumed: 42.2130 s\n",
      "Epoch: 270 Train_loss: 0.165 Val_acc: 0.889 Time consumed: 42.1179 s\n",
      "Epoch: 271 Train_loss: 0.160 Val_acc: 0.890 Time consumed: 41.8415 s\n",
      "Epoch: 272 Train_loss: 0.164 Val_acc: 0.894 Time consumed: 42.0288 s\n",
      "Epoch: 273 Train_loss: 0.173 Val_acc: 0.889 Time consumed: 42.1469 s\n",
      "Epoch: 274 Train_loss: 0.157 Val_acc: 0.890 Time consumed: 42.0972 s\n",
      "Epoch: 275 Train_loss: 0.162 Val_acc: 0.891 Time consumed: 42.1654 s\n",
      "Epoch: 276 Train_loss: 0.169 Val_acc: 0.893 Time consumed: 42.2870 s\n",
      "Epoch: 277 Train_loss: 0.160 Val_acc: 0.888 Time consumed: 42.2814 s\n",
      "Epoch: 278 Train_loss: 0.154 Val_acc: 0.892 Time consumed: 42.3894 s\n",
      "Epoch: 279 Train_loss: 0.162 Val_acc: 0.889 Time consumed: 42.0898 s\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "batch_gen = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "print('*****************2nd training stage*****************')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, save_dir+'cifar10.ckpt-183500')\n",
    "train_writer = tf.summary.FileWriter(save_dir+'poly/train', sess.graph)\n",
    "\n",
    "for m in range(epochs):\n",
    "        start = time.time()\n",
    "        for i in range(iterations):\n",
    "            x_batch, y_batch = next(batch_gen)\n",
    "            _, loss_train, summary = sess.run([train_op, loss, merged], \n",
    "                                     {inputs: x_batch, outputs: y_batch, is_train: True})\n",
    "        \n",
    "            train_writer.add_summary(summary, m*iterations + i + 1)\n",
    "\n",
    "        val_accs = []\n",
    "        for i in range(5000//(batch_size*5)):\n",
    "            val_acc = sess.run(accuracy, {inputs: x_test[i*batch_size*5: (i+1)*batch_size*5],\n",
    "                                          outputs: y_test[i*batch_size*5: (i+1)*batch_size*5],\n",
    "                                          is_train: False})\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        if np.mean(val_accs) > old_acc:\n",
    "            old_acc = np.mean(val_accs)\n",
    "            new_saver.save(sess, save_dir+'poly/cifar10.ckpt', global_step=global_step)\n",
    "\n",
    "        end = time.time()\n",
    "        print('Epoch: {}'.format(m + 1),\n",
    "              'Train_loss: {:.3f}'.format(loss_train),\n",
    "              'Val_acc: {:.3f}'.format(np.mean(val_accs)),\n",
    "              'Time consumed: {:.4f} s'.format(end - start))\n",
    "\n",
    "print('*****************Training End!*****************')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
