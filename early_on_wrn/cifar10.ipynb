{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./benchmark_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "%run ./datagen.py\n",
    "\n",
    "datagen, (x_train, y_train), (x_test, y_test) = data_preparation()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_logdir = './benchmark_resnet/'\n",
    "path_model = './benchmark_resnet/'\n",
    "num_gpu = 4\n",
    "batch_size = 100\n",
    "iterations = x_train.shape[0] // (batch_size * num_gpu)\n",
    "epochs = 500\n",
    "old_acc = 0\n",
    "start_lr = 1e-2\n",
    "end_lr = 5*1e-4\n",
    "decay_rate = (end_lr / start_lr) ** (1 / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(x):\n",
    "#     out = 0.5 + tf.multiply(0.133, x) - tf.multiply(0.0014, tf.pow(x, 3))\n",
    "    out = 0.5 + tf.multiply(0.15012, x) - tf.multiply(0.0015930078, tf.pow(x, 3))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet layer\n",
    "def res_layer(inputs, filter_num, filter_size, stride, is_train, \n",
    "              conv_first=False, batch_norm=True, activation=True):\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    if conv_first:\n",
    "        x = tf.layers.conv2d(inputs=x, filters=filter_num, \n",
    "                             kernel_size=filter_size, strides=stride, padding='same')\n",
    "        if batch_norm:\n",
    "            x = tf.layers.batch_normalization(x, training=is_train)\n",
    "        if activation:\n",
    "            x = act(x)\n",
    "    else:\n",
    "        if batch_norm:\n",
    "            x = tf.layers.batch_normalization(x, training=is_train)\n",
    "        if activation:\n",
    "            x = act(x)\n",
    "        x = tf.layers.conv2d(inputs=x, filters=filter_num, \n",
    "                             kernel_size=filter_size, strides=stride, padding='same')\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnetv2(inputs, is_train):\n",
    "    \n",
    "    with tf.variable_scope(\"Conv1\"):\n",
    "        x = res_layer(inputs, 64, 3, 2, is_train, conv_first=True)\n",
    "          \n",
    "    # Res Blocks\n",
    "    for stack in range(3):\n",
    "        for block in range(6):\n",
    "            with tf.variable_scope('ResBlock{}'.format(stack*6+block+1)):\n",
    "                \n",
    "                batch_norm = True\n",
    "                activation = True\n",
    "                stride = 1\n",
    "                if stack == 0:\n",
    "                    filter_num = 64\n",
    "                    if block == 0:\n",
    "                        batch_norm = False\n",
    "                        activation = False\n",
    "                else:\n",
    "                    filter_num = 64*2*stack\n",
    "                    if block == 0:\n",
    "                        stride = 2\n",
    "                \n",
    "                residual_x = x\n",
    "                with tf.variable_scope('conv1'):\n",
    "                    x = res_layer(x, filter_num, 1, stride, is_train, \n",
    "                                  batch_norm=batch_norm, activation=activation)\n",
    "                with tf.variable_scope('conv2'):\n",
    "                    x = res_layer(x, filter_num, 3, 1, is_train, \n",
    "                                  batch_norm=batch_norm, activation=activation)\n",
    "                with tf.variable_scope('conv3'):\n",
    "                    x = res_layer(x, filter_num*4, 1, 1, is_train, \n",
    "                                  batch_norm=batch_norm, activation=activation)\n",
    "                if block == 0:\n",
    "                    with tf.variable_scope('residual'):\n",
    "                        residual_x = res_layer(residual_x, filter_num*4, 1, stride, is_train, \n",
    "                                               batch_norm=False, activation=False)\n",
    "                x = x + residual_x   \n",
    "    \n",
    "    #x.shape = (?, 4, 4, 1024)\n",
    "    with tf.variable_scope(\"AfterResBlock\"):\n",
    "        x = tf.layers.batch_normalization(x, training=is_train)                \n",
    "        x = act(x)\n",
    "        x = 4 * tf.layers.average_pooling2d(x, pool_size=2, strides=2, padding='SAME', name='pool1')\n",
    "    \n",
    "    with tf.variable_scope(\"Flatten\"):\n",
    "        x = tf.transpose(x, perm=[0, 3, 1, 2])\n",
    "        x = tf.layers.flatten(x)\n",
    "    \n",
    "    with tf.variable_scope(\"Prediction\"):\n",
    "        pred = tf.layers.dense(x, units=10)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inference(inputs, is_train):\n",
    "\n",
    "#     # L1: 2*128conv + pooling + bn\n",
    "#     with tf.variable_scope(\"ConvBlock1\"):\n",
    "#         x = tf.layers.conv2d(inputs=inputs, \n",
    "#                    filters=128, \n",
    "#                    kernel_size=(3, 3),\n",
    "#                    strides=(1, 1), \n",
    "#                    padding='same')\n",
    "#         x = tf.layers.batch_normalization(x, training=is_train)\n",
    "#         x = tf.nn.relu(x)\n",
    "\n",
    "#         x = tf.layers.conv2d(inputs=x,\n",
    "#                    filters=128, \n",
    "#                    kernel_size=(3, 3), \n",
    "#                    strides=(1, 1), \n",
    "#                    padding='same')\n",
    "#         x = 4 * tf.layers.average_pooling2d(x, pool_size=(2, 2), strides=(2, 2), padding='SAME')\n",
    "#         x = tf.layers.batch_normalization(x, training=is_train)        \n",
    "#         x = tf.nn.relu(x)\n",
    "# #         x = tf.layers.dropout(x, 0.1)\n",
    "\n",
    "#     # L2: 2*256conv + pooling + bn + dropout\n",
    "#     with tf.variable_scope(\"ConvBlock2\"):\n",
    "#         x = tf.layers.conv2d(inputs=x,\n",
    "#                    filters=256, \n",
    "#                    kernel_size=(3, 3), \n",
    "#                    strides=(1, 1), \n",
    "#                    padding='same')\n",
    "#         x = tf.layers.batch_normalization(x, training=is_train)        \n",
    "#         x = tf.nn.relu(x)\n",
    "\n",
    "#         x = tf.layers.conv2d(inputs=x,\n",
    "#                    filters=256, \n",
    "#                    kernel_size=(3, 3), \n",
    "#                    strides=(1, 1), \n",
    "#                    padding='same')\n",
    "#         x = 4*tf.layers.average_pooling2d(x, pool_size=(2, 2), strides=(2, 2), padding='SAME')\n",
    "#         x = tf.layers.batch_normalization(x, training=is_train)\n",
    "#         x = tf.nn.relu(x)\n",
    "# #         x = tf.layers.dropout(x, 0.1)\n",
    "\n",
    "#     # L3: 2*512conv + pooling + dropout\n",
    "#     with tf.variable_scope(\"ConvBlock3\"):\n",
    "#         x = tf.layers.conv2d(inputs=x,\n",
    "#                    filters=512, \n",
    "#                    kernel_size=(3, 3), \n",
    "#                    strides=(1, 1), \n",
    "#                    padding='same')\n",
    "#         x = tf.layers.batch_normalization(x, training=is_train)\n",
    "#         x = tf.nn.relu(x)\n",
    "        \n",
    "#         x = tf.layers.conv2d(inputs=x,\n",
    "#                    filters=512, \n",
    "#                    kernel_size=(3, 3), \n",
    "#                    strides=(1, 1), \n",
    "#                    padding='same')\n",
    "#         x = 4*tf.layers.average_pooling2d(x, pool_size=(2, 2), strides=(2, 2), padding='SAME')\n",
    "#         x = tf.layers.batch_normalization(x, training=is_train)\n",
    "#         x = tf.nn.relu(x)    \n",
    "# #         x = tf.layers.dropout(x, 0.1)\n",
    "\n",
    "#     with tf.variable_scope(\"Flatten\"):\n",
    "#         x = tf.transpose(x, perm=[0, 3, 1, 2])\n",
    "#         x = tf.layers.flatten(x)\n",
    "\n",
    "#     # L4: 2*FC1024 + bn + dropout\n",
    "#     with tf.variable_scope(\"FCBlock1\"):\n",
    "#         x = tf.layers.dense(x, units=1024)\n",
    "#         x = tf.layers.batch_normalization(x, training=is_train)       \n",
    "#         x = tf.nn.relu(x)\n",
    "\n",
    "#         x = tf.layers.dense(x, units=1024)\n",
    "#         x = tf.layers.batch_normalization(x, training=is_train)\n",
    "#         x = tf.nn.relu(x)   \n",
    "# #         x = tf.layers.dropout(x, 0.1)\n",
    "\n",
    "#     # predict layer\n",
    "#     with tf.variable_scope(\"Prediction\"):\n",
    "#         pred = tf.layers.dense(x, units=10)\n",
    "    \n",
    "#     return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            expend_g = tf.expand_dims(g, 0)\n",
    "            grads.append(expend_g)\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-c3acf23a4461>:9: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-c3acf23a4461>:11: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-8e415ca65875>:44: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.average_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-8e415ca65875>:48: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-8e415ca65875>:51: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, [None, 32, 32, 3], name='input')\n",
    "    outputs = tf.placeholder(tf.float32, [None, 10], name='output')\n",
    "    is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    l_r = tf.train.exponential_decay(start_lr, global_step, iterations, decay_rate, staircase=True)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=l_r)\n",
    "\n",
    "    tower_grads = []\n",
    "    tower_acc = []\n",
    "    tower_loss = []\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope()) as variable_scope:\n",
    "        for i in range(num_gpu):\n",
    "            with tf.device('/gpu:%d' % (i+5)):\n",
    "                with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                    x = inputs[i * batch_size:(i + 1) * batch_size]\n",
    "                    y = outputs[i * batch_size:(i + 1) * batch_size]\n",
    "                    pred = resnetv2(x, is_train)\n",
    "                    \n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    \n",
    "                    loss = tf.losses.softmax_cross_entropy(y, pred)\n",
    "                    tower_loss.append(loss)\n",
    "#                     loss = tf.reduce_mean(tf.nn.relu(tf.losses.hinge_loss(y, pred)))\n",
    "                    grads = opt.compute_gradients(loss)\n",
    "                    tower_grads.append(grads)\n",
    "\n",
    "                    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "                    tower_acc.append(accuracy)\n",
    "    \n",
    "    losses = tf.reduce_mean(tower_loss)\n",
    "    grads = average_gradients(tower_grads)\n",
    "    accs = tf.reduce_mean(tower_acc)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#     print(update_ops)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    tf.summary.scalar('loss', losses)\n",
    "    tf.summary.scalar('accuracy', accs)\n",
    "    tf.summary.scalar('learning_rate', l_r)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Training Start!*****************\n",
      "Epoch: 1 Train_loss: 17.275 Val_acc: 0.153 Time consumed: 99.6743 s\n",
      "Epoch: 2 Train_loss: 23.583 Val_acc: 0.193 Time consumed: 56.4696 s\n",
      "Epoch: 3 Train_loss: 13.616 Val_acc: 0.172 Time consumed: 52.7602 s\n",
      "Epoch: 4 Train_loss: 6.844 Val_acc: 0.126 Time consumed: 52.7744 s\n",
      "Epoch: 5 Train_loss: 5.061 Val_acc: 0.170 Time consumed: 52.8774 s\n",
      "Epoch: 6 Train_loss: 6.090 Val_acc: 0.203 Time consumed: 56.7235 s\n",
      "Epoch: 7 Train_loss: 2.924 Val_acc: 0.225 Time consumed: 57.2063 s\n",
      "Epoch: 8 Train_loss: 2.101 Val_acc: 0.292 Time consumed: 57.5051 s\n",
      "Epoch: 9 Train_loss: 2.475 Val_acc: 0.169 Time consumed: 53.5115 s\n",
      "Epoch: 10 Train_loss: 2.309 Val_acc: 0.209 Time consumed: 52.5538 s\n",
      "Epoch: 11 Train_loss: 2.568 Val_acc: 0.213 Time consumed: 52.7418 s\n",
      "Epoch: 12 Train_loss: 3.240 Val_acc: 0.275 Time consumed: 53.0867 s\n",
      "Epoch: 13 Train_loss: 1.957 Val_acc: 0.285 Time consumed: 53.8268 s\n",
      "Epoch: 14 Train_loss: 2.183 Val_acc: 0.239 Time consumed: 51.1126 s\n",
      "Epoch: 15 Train_loss: 2.111 Val_acc: 0.244 Time consumed: 51.1105 s\n",
      "Epoch: 16 Train_loss: 2.043 Val_acc: 0.209 Time consumed: 50.3955 s\n",
      "Epoch: 17 Train_loss: 2.629 Val_acc: 0.213 Time consumed: 49.9606 s\n",
      "Epoch: 18 Train_loss: 1.802 Val_acc: 0.322 Time consumed: 55.2456 s\n",
      "Epoch: 19 Train_loss: 2.027 Val_acc: 0.323 Time consumed: 54.6724 s\n",
      "Epoch: 20 Train_loss: 1.833 Val_acc: 0.308 Time consumed: 49.4777 s\n",
      "Epoch: 21 Train_loss: 1.998 Val_acc: 0.346 Time consumed: 53.7808 s\n",
      "Epoch: 22 Train_loss: 1.825 Val_acc: 0.326 Time consumed: 50.0910 s\n",
      "Epoch: 23 Train_loss: 1.773 Val_acc: 0.351 Time consumed: 53.5841 s\n",
      "Epoch: 24 Train_loss: 1.927 Val_acc: 0.308 Time consumed: 49.6446 s\n",
      "Epoch: 25 Train_loss: 1.804 Val_acc: 0.302 Time consumed: 49.8449 s\n",
      "Epoch: 26 Train_loss: 1.787 Val_acc: 0.348 Time consumed: 49.9019 s\n",
      "Epoch: 27 Train_loss: 1.795 Val_acc: 0.332 Time consumed: 49.6454 s\n",
      "Epoch: 28 Train_loss: 1.859 Val_acc: 0.325 Time consumed: 49.7572 s\n",
      "Epoch: 29 Train_loss: 1.638 Val_acc: 0.369 Time consumed: 53.7585 s\n",
      "Epoch: 30 Train_loss: 1.666 Val_acc: 0.384 Time consumed: 53.4304 s\n",
      "Epoch: 31 Train_loss: 1.616 Val_acc: 0.368 Time consumed: 49.4745 s\n",
      "Epoch: 32 Train_loss: 1.749 Val_acc: 0.349 Time consumed: 49.5850 s\n",
      "Epoch: 33 Train_loss: 1.674 Val_acc: 0.373 Time consumed: 49.5720 s\n",
      "Epoch: 34 Train_loss: 1.620 Val_acc: 0.381 Time consumed: 49.5841 s\n",
      "Epoch: 35 Train_loss: 1.682 Val_acc: 0.406 Time consumed: 53.9692 s\n",
      "Epoch: 36 Train_loss: 1.654 Val_acc: 0.404 Time consumed: 49.6825 s\n",
      "Epoch: 37 Train_loss: 1.502 Val_acc: 0.354 Time consumed: 49.4508 s\n",
      "Epoch: 38 Train_loss: 1.622 Val_acc: 0.401 Time consumed: 49.4722 s\n",
      "Epoch: 39 Train_loss: 1.645 Val_acc: 0.372 Time consumed: 50.2141 s\n",
      "Epoch: 40 Train_loss: 1.651 Val_acc: 0.372 Time consumed: 50.0641 s\n",
      "Epoch: 41 Train_loss: 1.603 Val_acc: 0.383 Time consumed: 49.9822 s\n",
      "Epoch: 42 Train_loss: 1.684 Val_acc: 0.311 Time consumed: 50.0351 s\n",
      "Epoch: 43 Train_loss: 1.567 Val_acc: 0.422 Time consumed: 53.8363 s\n",
      "Epoch: 44 Train_loss: 1.510 Val_acc: 0.396 Time consumed: 49.7272 s\n",
      "Epoch: 45 Train_loss: 1.651 Val_acc: 0.401 Time consumed: 49.8576 s\n",
      "Epoch: 46 Train_loss: 1.644 Val_acc: 0.409 Time consumed: 49.7492 s\n",
      "Epoch: 47 Train_loss: 1.624 Val_acc: 0.373 Time consumed: 49.8977 s\n",
      "Epoch: 48 Train_loss: 1.613 Val_acc: 0.365 Time consumed: 49.5808 s\n",
      "Epoch: 49 Train_loss: 1.582 Val_acc: 0.427 Time consumed: 54.0576 s\n",
      "Epoch: 50 Train_loss: 1.531 Val_acc: 0.407 Time consumed: 49.7546 s\n",
      "Epoch: 51 Train_loss: 1.463 Val_acc: 0.414 Time consumed: 49.6673 s\n",
      "Epoch: 52 Train_loss: 1.471 Val_acc: 0.411 Time consumed: 49.9836 s\n",
      "Epoch: 53 Train_loss: 1.506 Val_acc: 0.408 Time consumed: 49.9351 s\n",
      "Epoch: 54 Train_loss: 1.515 Val_acc: 0.435 Time consumed: 53.6514 s\n",
      "Epoch: 55 Train_loss: 1.527 Val_acc: 0.419 Time consumed: 49.5548 s\n",
      "Epoch: 56 Train_loss: 1.544 Val_acc: 0.428 Time consumed: 49.9015 s\n",
      "Epoch: 57 Train_loss: 1.493 Val_acc: 0.414 Time consumed: 50.0093 s\n",
      "Epoch: 58 Train_loss: 1.501 Val_acc: 0.465 Time consumed: 55.4168 s\n",
      "Epoch: 59 Train_loss: 1.549 Val_acc: 0.401 Time consumed: 49.9354 s\n",
      "Epoch: 60 Train_loss: 1.488 Val_acc: 0.429 Time consumed: 49.7290 s\n",
      "Epoch: 61 Train_loss: 1.442 Val_acc: 0.428 Time consumed: 49.5858 s\n",
      "Epoch: 62 Train_loss: 1.404 Val_acc: 0.376 Time consumed: 49.9650 s\n",
      "Epoch: 63 Train_loss: 1.450 Val_acc: 0.285 Time consumed: 49.8503 s\n",
      "Epoch: 64 Train_loss: 1.574 Val_acc: 0.448 Time consumed: 49.7696 s\n",
      "Epoch: 65 Train_loss: 1.469 Val_acc: 0.429 Time consumed: 49.5304 s\n",
      "Epoch: 66 Train_loss: 1.550 Val_acc: 0.420 Time consumed: 49.7499 s\n",
      "Epoch: 67 Train_loss: 1.509 Val_acc: 0.430 Time consumed: 49.7960 s\n",
      "Epoch: 68 Train_loss: 1.518 Val_acc: 0.371 Time consumed: 49.9563 s\n",
      "Epoch: 69 Train_loss: 1.494 Val_acc: 0.377 Time consumed: 49.4020 s\n",
      "Epoch: 70 Train_loss: 1.473 Val_acc: 0.426 Time consumed: 50.0113 s\n",
      "Epoch: 71 Train_loss: 1.409 Val_acc: 0.393 Time consumed: 49.7477 s\n",
      "Epoch: 72 Train_loss: 1.420 Val_acc: 0.444 Time consumed: 49.8903 s\n",
      "Epoch: 73 Train_loss: 1.421 Val_acc: 0.447 Time consumed: 49.8765 s\n",
      "Epoch: 74 Train_loss: 1.422 Val_acc: 0.363 Time consumed: 49.8498 s\n",
      "Epoch: 75 Train_loss: 1.581 Val_acc: 0.439 Time consumed: 49.5710 s\n",
      "Epoch: 76 Train_loss: 1.434 Val_acc: 0.392 Time consumed: 49.8062 s\n",
      "Epoch: 77 Train_loss: 1.492 Val_acc: 0.295 Time consumed: 49.7541 s\n",
      "Epoch: 78 Train_loss: 1.429 Val_acc: 0.462 Time consumed: 49.6647 s\n",
      "Epoch: 79 Train_loss: 1.410 Val_acc: 0.472 Time consumed: 54.6663 s\n",
      "Epoch: 80 Train_loss: 1.468 Val_acc: 0.348 Time consumed: 49.5106 s\n",
      "Epoch: 81 Train_loss: 1.485 Val_acc: 0.442 Time consumed: 49.4585 s\n",
      "Epoch: 82 Train_loss: 1.389 Val_acc: 0.438 Time consumed: 49.8218 s\n",
      "Epoch: 83 Train_loss: 1.582 Val_acc: 0.384 Time consumed: 49.6073 s\n",
      "Epoch: 84 Train_loss: 1.446 Val_acc: 0.394 Time consumed: 49.5077 s\n",
      "Epoch: 85 Train_loss: 1.473 Val_acc: 0.451 Time consumed: 49.9502 s\n",
      "Epoch: 86 Train_loss: 1.390 Val_acc: 0.464 Time consumed: 50.1831 s\n",
      "Epoch: 87 Train_loss: 1.381 Val_acc: 0.460 Time consumed: 49.9859 s\n",
      "Epoch: 88 Train_loss: 1.480 Val_acc: 0.465 Time consumed: 49.4367 s\n",
      "Epoch: 89 Train_loss: 1.492 Val_acc: 0.446 Time consumed: 49.6582 s\n",
      "Epoch: 90 Train_loss: 1.433 Val_acc: 0.460 Time consumed: 49.8423 s\n",
      "Epoch: 91 Train_loss: 1.373 Val_acc: 0.404 Time consumed: 49.5277 s\n",
      "Epoch: 92 Train_loss: 1.400 Val_acc: 0.383 Time consumed: 49.8618 s\n",
      "Epoch: 93 Train_loss: 1.454 Val_acc: 0.443 Time consumed: 49.7660 s\n",
      "Epoch: 94 Train_loss: 1.420 Val_acc: 0.435 Time consumed: 49.7422 s\n",
      "Epoch: 95 Train_loss: 1.451 Val_acc: 0.467 Time consumed: 50.1497 s\n",
      "Epoch: 96 Train_loss: 1.371 Val_acc: 0.471 Time consumed: 49.8602 s\n",
      "Epoch: 97 Train_loss: 1.427 Val_acc: 0.436 Time consumed: 49.9169 s\n",
      "Epoch: 98 Train_loss: 1.415 Val_acc: 0.422 Time consumed: 49.8735 s\n",
      "Epoch: 99 Train_loss: 1.504 Val_acc: 0.477 Time consumed: 54.6867 s\n",
      "Epoch: 100 Train_loss: 1.446 Val_acc: 0.459 Time consumed: 49.5652 s\n",
      "Epoch: 101 Train_loss: 1.355 Val_acc: 0.457 Time consumed: 49.8249 s\n",
      "Epoch: 102 Train_loss: 1.451 Val_acc: 0.450 Time consumed: 49.7969 s\n",
      "Epoch: 103 Train_loss: 1.465 Val_acc: 0.465 Time consumed: 49.7988 s\n",
      "Epoch: 104 Train_loss: 1.510 Val_acc: 0.470 Time consumed: 49.6460 s\n",
      "Epoch: 105 Train_loss: 1.511 Val_acc: 0.409 Time consumed: 50.0967 s\n",
      "Epoch: 106 Train_loss: 1.535 Val_acc: 0.447 Time consumed: 49.3740 s\n",
      "Epoch: 107 Train_loss: 1.359 Val_acc: 0.442 Time consumed: 49.8717 s\n",
      "Epoch: 108 Train_loss: 1.486 Val_acc: 0.451 Time consumed: 49.9762 s\n",
      "Epoch: 109 Train_loss: 1.402 Val_acc: 0.472 Time consumed: 50.0002 s\n",
      "Epoch: 110 Train_loss: 1.394 Val_acc: 0.477 Time consumed: 49.9252 s\n",
      "Epoch: 111 Train_loss: 1.419 Val_acc: 0.473 Time consumed: 49.8081 s\n",
      "Epoch: 112 Train_loss: 1.400 Val_acc: 0.455 Time consumed: 49.7243 s\n",
      "Epoch: 113 Train_loss: 1.434 Val_acc: 0.480 Time consumed: 53.6507 s\n",
      "Epoch: 114 Train_loss: 1.395 Val_acc: 0.425 Time consumed: 49.6398 s\n",
      "Epoch: 115 Train_loss: 1.357 Val_acc: 0.484 Time consumed: 53.6224 s\n",
      "Epoch: 116 Train_loss: 1.425 Val_acc: 0.468 Time consumed: 49.8048 s\n",
      "Epoch: 117 Train_loss: 1.425 Val_acc: 0.471 Time consumed: 49.8550 s\n",
      "Epoch: 118 Train_loss: 1.455 Val_acc: 0.475 Time consumed: 49.6819 s\n",
      "Epoch: 119 Train_loss: 1.377 Val_acc: 0.486 Time consumed: 54.1175 s\n",
      "Epoch: 120 Train_loss: 1.335 Val_acc: 0.476 Time consumed: 49.5024 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121 Train_loss: 1.408 Val_acc: 0.490 Time consumed: 53.7729 s\n",
      "Epoch: 122 Train_loss: 1.445 Val_acc: 0.367 Time consumed: 49.5391 s\n",
      "Epoch: 123 Train_loss: 1.409 Val_acc: 0.423 Time consumed: 49.7690 s\n",
      "Epoch: 124 Train_loss: 1.374 Val_acc: 0.486 Time consumed: 49.5165 s\n",
      "Epoch: 125 Train_loss: 1.402 Val_acc: 0.379 Time consumed: 49.7055 s\n",
      "Epoch: 126 Train_loss: 1.405 Val_acc: 0.459 Time consumed: 49.8529 s\n",
      "Epoch: 127 Train_loss: 1.444 Val_acc: 0.429 Time consumed: 49.8329 s\n",
      "Epoch: 128 Train_loss: 1.286 Val_acc: 0.473 Time consumed: 49.8932 s\n",
      "Epoch: 129 Train_loss: 1.428 Val_acc: 0.491 Time consumed: 54.0098 s\n",
      "Epoch: 130 Train_loss: 1.419 Val_acc: 0.448 Time consumed: 49.8179 s\n",
      "Epoch: 131 Train_loss: 1.420 Val_acc: 0.461 Time consumed: 49.8512 s\n",
      "Epoch: 132 Train_loss: 1.305 Val_acc: 0.479 Time consumed: 50.0178 s\n",
      "Epoch: 133 Train_loss: 1.388 Val_acc: 0.483 Time consumed: 49.6348 s\n",
      "Epoch: 134 Train_loss: 1.434 Val_acc: 0.473 Time consumed: 49.8702 s\n",
      "Epoch: 135 Train_loss: 1.386 Val_acc: 0.416 Time consumed: 50.0410 s\n",
      "Epoch: 136 Train_loss: 1.470 Val_acc: 0.484 Time consumed: 49.9810 s\n",
      "Epoch: 137 Train_loss: 1.296 Val_acc: 0.484 Time consumed: 49.4457 s\n",
      "Epoch: 138 Train_loss: 1.426 Val_acc: 0.476 Time consumed: 49.6919 s\n",
      "Epoch: 139 Train_loss: 1.433 Val_acc: 0.493 Time consumed: 54.5207 s\n",
      "Epoch: 140 Train_loss: 1.309 Val_acc: 0.477 Time consumed: 49.5113 s\n",
      "Epoch: 141 Train_loss: 1.303 Val_acc: 0.476 Time consumed: 50.2200 s\n",
      "Epoch: 142 Train_loss: 1.349 Val_acc: 0.483 Time consumed: 49.8869 s\n",
      "Epoch: 143 Train_loss: 1.371 Val_acc: 0.464 Time consumed: 49.7999 s\n",
      "Epoch: 144 Train_loss: 1.436 Val_acc: 0.462 Time consumed: 49.6808 s\n",
      "Epoch: 145 Train_loss: 1.520 Val_acc: 0.464 Time consumed: 49.6611 s\n",
      "Epoch: 146 Train_loss: 1.331 Val_acc: 0.454 Time consumed: 49.7367 s\n",
      "Epoch: 147 Train_loss: 1.402 Val_acc: 0.488 Time consumed: 50.0139 s\n",
      "Epoch: 148 Train_loss: 1.454 Val_acc: 0.424 Time consumed: 49.9299 s\n",
      "Epoch: 149 Train_loss: 1.297 Val_acc: 0.483 Time consumed: 49.3468 s\n",
      "Epoch: 150 Train_loss: 1.386 Val_acc: 0.486 Time consumed: 49.7396 s\n",
      "Epoch: 151 Train_loss: 1.455 Val_acc: 0.497 Time consumed: 53.9232 s\n",
      "Epoch: 152 Train_loss: 1.494 Val_acc: 0.415 Time consumed: 49.7257 s\n",
      "Epoch: 153 Train_loss: 1.415 Val_acc: 0.487 Time consumed: 49.9337 s\n",
      "Epoch: 154 Train_loss: 1.295 Val_acc: 0.481 Time consumed: 49.6587 s\n",
      "Epoch: 155 Train_loss: 1.399 Val_acc: 0.482 Time consumed: 49.8996 s\n",
      "Epoch: 156 Train_loss: 1.402 Val_acc: 0.476 Time consumed: 49.7696 s\n",
      "Epoch: 157 Train_loss: 1.290 Val_acc: 0.491 Time consumed: 50.0602 s\n",
      "Epoch: 158 Train_loss: 1.471 Val_acc: 0.441 Time consumed: 49.5226 s\n",
      "Epoch: 159 Train_loss: 1.442 Val_acc: 0.483 Time consumed: 50.0485 s\n",
      "Epoch: 160 Train_loss: 1.318 Val_acc: 0.478 Time consumed: 49.9454 s\n",
      "Epoch: 161 Train_loss: 1.348 Val_acc: 0.478 Time consumed: 49.8660 s\n",
      "Epoch: 162 Train_loss: 1.356 Val_acc: 0.481 Time consumed: 50.1739 s\n",
      "Epoch: 163 Train_loss: 1.283 Val_acc: 0.499 Time consumed: 54.2090 s\n",
      "Epoch: 164 Train_loss: 1.328 Val_acc: 0.463 Time consumed: 49.5756 s\n",
      "Epoch: 165 Train_loss: 1.446 Val_acc: 0.492 Time consumed: 49.9362 s\n",
      "Epoch: 166 Train_loss: 1.408 Val_acc: 0.482 Time consumed: 49.7307 s\n",
      "Epoch: 167 Train_loss: 1.460 Val_acc: 0.469 Time consumed: 49.8410 s\n",
      "Epoch: 168 Train_loss: 1.304 Val_acc: 0.502 Time consumed: 53.4797 s\n",
      "Epoch: 169 Train_loss: 1.424 Val_acc: 0.443 Time consumed: 49.5732 s\n",
      "Epoch: 170 Train_loss: 1.336 Val_acc: 0.451 Time consumed: 49.9477 s\n",
      "Epoch: 171 Train_loss: 1.326 Val_acc: 0.485 Time consumed: 50.0526 s\n",
      "Epoch: 172 Train_loss: 1.312 Val_acc: 0.470 Time consumed: 50.0409 s\n",
      "Epoch: 173 Train_loss: 1.365 Val_acc: 0.484 Time consumed: 49.7646 s\n",
      "Epoch: 174 Train_loss: 1.409 Val_acc: 0.473 Time consumed: 49.9362 s\n",
      "Epoch: 175 Train_loss: 1.437 Val_acc: 0.481 Time consumed: 49.9259 s\n",
      "Epoch: 176 Train_loss: 1.298 Val_acc: 0.494 Time consumed: 49.8898 s\n",
      "Epoch: 177 Train_loss: 1.392 Val_acc: 0.495 Time consumed: 49.6187 s\n",
      "Epoch: 178 Train_loss: 1.270 Val_acc: 0.497 Time consumed: 49.9445 s\n",
      "Epoch: 179 Train_loss: 1.482 Val_acc: 0.478 Time consumed: 50.1980 s\n",
      "Epoch: 180 Train_loss: 1.389 Val_acc: 0.476 Time consumed: 49.8845 s\n",
      "Epoch: 181 Train_loss: 1.386 Val_acc: 0.497 Time consumed: 50.2536 s\n",
      "Epoch: 182 Train_loss: 1.327 Val_acc: 0.506 Time consumed: 54.5266 s\n",
      "Epoch: 183 Train_loss: 1.341 Val_acc: 0.493 Time consumed: 49.7289 s\n",
      "Epoch: 184 Train_loss: 1.343 Val_acc: 0.471 Time consumed: 49.8215 s\n",
      "Epoch: 185 Train_loss: 1.298 Val_acc: 0.458 Time consumed: 49.7083 s\n",
      "Epoch: 186 Train_loss: 1.353 Val_acc: 0.501 Time consumed: 49.9450 s\n",
      "Epoch: 187 Train_loss: 1.371 Val_acc: 0.469 Time consumed: 49.9501 s\n",
      "Epoch: 188 Train_loss: 1.323 Val_acc: 0.480 Time consumed: 49.8366 s\n",
      "Epoch: 189 Train_loss: 1.308 Val_acc: 0.496 Time consumed: 49.7456 s\n",
      "Epoch: 190 Train_loss: 1.369 Val_acc: 0.475 Time consumed: 49.7909 s\n",
      "Epoch: 191 Train_loss: 1.443 Val_acc: 0.504 Time consumed: 49.7520 s\n",
      "Epoch: 192 Train_loss: 1.310 Val_acc: 0.501 Time consumed: 49.9101 s\n",
      "Epoch: 193 Train_loss: 1.429 Val_acc: 0.461 Time consumed: 49.9195 s\n",
      "Epoch: 194 Train_loss: 1.307 Val_acc: 0.499 Time consumed: 49.7620 s\n",
      "Epoch: 195 Train_loss: 1.281 Val_acc: 0.495 Time consumed: 50.0023 s\n",
      "Epoch: 196 Train_loss: 1.311 Val_acc: 0.446 Time consumed: 49.7560 s\n",
      "Epoch: 197 Train_loss: 1.488 Val_acc: 0.229 Time consumed: 50.0636 s\n",
      "Epoch: 198 Train_loss: 1.458 Val_acc: 0.507 Time consumed: 54.2421 s\n",
      "Epoch: 199 Train_loss: 1.382 Val_acc: 0.490 Time consumed: 50.2056 s\n",
      "Epoch: 200 Train_loss: 1.454 Val_acc: 0.468 Time consumed: 49.8645 s\n",
      "Epoch: 201 Train_loss: 1.351 Val_acc: 0.449 Time consumed: 49.7614 s\n",
      "Epoch: 202 Train_loss: 1.302 Val_acc: 0.485 Time consumed: 50.5148 s\n",
      "Epoch: 203 Train_loss: 1.352 Val_acc: 0.484 Time consumed: 50.0268 s\n",
      "Epoch: 204 Train_loss: 1.384 Val_acc: 0.495 Time consumed: 49.7714 s\n",
      "Epoch: 205 Train_loss: 1.238 Val_acc: 0.447 Time consumed: 49.7845 s\n",
      "Epoch: 206 Train_loss: 1.285 Val_acc: 0.497 Time consumed: 49.7096 s\n",
      "Epoch: 207 Train_loss: 1.325 Val_acc: 0.501 Time consumed: 49.8367 s\n",
      "Epoch: 208 Train_loss: 1.331 Val_acc: 0.450 Time consumed: 49.7200 s\n",
      "Epoch: 209 Train_loss: 1.398 Val_acc: 0.485 Time consumed: 49.8574 s\n",
      "Epoch: 210 Train_loss: 1.285 Val_acc: 0.477 Time consumed: 49.8581 s\n",
      "Epoch: 211 Train_loss: 1.266 Val_acc: 0.484 Time consumed: 49.7390 s\n",
      "Epoch: 212 Train_loss: 1.382 Val_acc: 0.464 Time consumed: 49.8139 s\n",
      "Epoch: 213 Train_loss: 1.425 Val_acc: 0.448 Time consumed: 49.8034 s\n",
      "Epoch: 214 Train_loss: 1.412 Val_acc: 0.491 Time consumed: 49.9082 s\n",
      "Epoch: 215 Train_loss: 1.413 Val_acc: 0.493 Time consumed: 49.9828 s\n",
      "Epoch: 216 Train_loss: 1.469 Val_acc: 0.499 Time consumed: 49.8263 s\n",
      "Epoch: 217 Train_loss: 1.348 Val_acc: 0.482 Time consumed: 49.7848 s\n",
      "Epoch: 218 Train_loss: 1.316 Val_acc: 0.503 Time consumed: 49.8836 s\n",
      "Epoch: 219 Train_loss: 1.437 Val_acc: 0.452 Time consumed: 50.2030 s\n",
      "Epoch: 220 Train_loss: 1.414 Val_acc: 0.507 Time consumed: 49.8870 s\n",
      "Epoch: 221 Train_loss: 1.365 Val_acc: 0.470 Time consumed: 49.7863 s\n",
      "Epoch: 222 Train_loss: 1.371 Val_acc: 0.493 Time consumed: 49.8539 s\n",
      "Epoch: 223 Train_loss: 1.313 Val_acc: 0.485 Time consumed: 50.0332 s\n",
      "Epoch: 224 Train_loss: 1.342 Val_acc: 0.486 Time consumed: 49.8351 s\n",
      "Epoch: 225 Train_loss: 1.470 Val_acc: 0.266 Time consumed: 49.9826 s\n",
      "Epoch: 226 Train_loss: 1.373 Val_acc: 0.481 Time consumed: 49.9483 s\n",
      "Epoch: 227 Train_loss: 1.334 Val_acc: 0.494 Time consumed: 50.2398 s\n",
      "Epoch: 228 Train_loss: 1.359 Val_acc: 0.474 Time consumed: 50.2287 s\n",
      "Epoch: 229 Train_loss: 1.344 Val_acc: 0.498 Time consumed: 50.4597 s\n",
      "Epoch: 230 Train_loss: 1.270 Val_acc: 0.502 Time consumed: 50.5231 s\n",
      "Epoch: 231 Train_loss: 1.378 Val_acc: 0.502 Time consumed: 50.9609 s\n",
      "Epoch: 232 Train_loss: 1.274 Val_acc: 0.512 Time consumed: 53.9235 s\n",
      "Epoch: 233 Train_loss: 1.352 Val_acc: 0.487 Time consumed: 49.7328 s\n",
      "Epoch: 234 Train_loss: 1.349 Val_acc: 0.501 Time consumed: 49.7373 s\n",
      "Epoch: 235 Train_loss: 1.335 Val_acc: 0.493 Time consumed: 49.6300 s\n",
      "Epoch: 236 Train_loss: 1.329 Val_acc: 0.487 Time consumed: 49.7413 s\n",
      "Epoch: 237 Train_loss: 1.363 Val_acc: 0.489 Time consumed: 49.8367 s\n",
      "Epoch: 238 Train_loss: 1.430 Val_acc: 0.506 Time consumed: 50.0322 s\n",
      "Epoch: 239 Train_loss: 1.400 Val_acc: 0.485 Time consumed: 49.8164 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240 Train_loss: 1.330 Val_acc: 0.490 Time consumed: 49.7941 s\n",
      "Epoch: 241 Train_loss: 1.303 Val_acc: 0.469 Time consumed: 50.4371 s\n",
      "Epoch: 242 Train_loss: 1.377 Val_acc: 0.487 Time consumed: 50.0857 s\n",
      "Epoch: 243 Train_loss: 1.335 Val_acc: 0.493 Time consumed: 49.9199 s\n",
      "Epoch: 244 Train_loss: 1.355 Val_acc: 0.480 Time consumed: 50.3705 s\n",
      "Epoch: 245 Train_loss: 1.293 Val_acc: 0.474 Time consumed: 49.7327 s\n",
      "Epoch: 246 Train_loss: 1.390 Val_acc: 0.493 Time consumed: 49.9281 s\n",
      "Epoch: 247 Train_loss: 1.359 Val_acc: 0.492 Time consumed: 49.8771 s\n",
      "Epoch: 248 Train_loss: 1.341 Val_acc: 0.484 Time consumed: 50.3819 s\n",
      "Epoch: 249 Train_loss: 1.245 Val_acc: 0.490 Time consumed: 50.2625 s\n",
      "Epoch: 250 Train_loss: 1.387 Val_acc: 0.476 Time consumed: 49.8972 s\n",
      "Epoch: 251 Train_loss: 1.337 Val_acc: 0.489 Time consumed: 49.7912 s\n",
      "Epoch: 252 Train_loss: 1.346 Val_acc: 0.493 Time consumed: 49.9778 s\n",
      "Epoch: 253 Train_loss: 1.327 Val_acc: 0.510 Time consumed: 49.9682 s\n",
      "Epoch: 254 Train_loss: 1.312 Val_acc: 0.488 Time consumed: 50.1490 s\n",
      "Epoch: 255 Train_loss: 1.334 Val_acc: 0.487 Time consumed: 50.1441 s\n",
      "Epoch: 256 Train_loss: 1.304 Val_acc: 0.504 Time consumed: 49.8341 s\n",
      "Epoch: 257 Train_loss: 1.305 Val_acc: 0.504 Time consumed: 50.1580 s\n",
      "Epoch: 258 Train_loss: 1.400 Val_acc: 0.501 Time consumed: 50.0902 s\n",
      "Epoch: 259 Train_loss: 1.331 Val_acc: 0.497 Time consumed: 49.6669 s\n",
      "Epoch: 260 Train_loss: 1.459 Val_acc: 0.253 Time consumed: 49.8837 s\n",
      "Epoch: 261 Train_loss: 1.356 Val_acc: 0.501 Time consumed: 50.2574 s\n",
      "Epoch: 262 Train_loss: 1.323 Val_acc: 0.470 Time consumed: 50.4399 s\n",
      "Epoch: 263 Train_loss: 1.247 Val_acc: 0.488 Time consumed: 50.1267 s\n",
      "Epoch: 264 Train_loss: 1.431 Val_acc: 0.494 Time consumed: 50.2330 s\n",
      "Epoch: 265 Train_loss: 1.454 Val_acc: 0.508 Time consumed: 50.4005 s\n",
      "Epoch: 266 Train_loss: 1.330 Val_acc: 0.511 Time consumed: 51.4777 s\n",
      "Epoch: 267 Train_loss: 1.305 Val_acc: 0.503 Time consumed: 49.7593 s\n",
      "Epoch: 268 Train_loss: 1.342 Val_acc: 0.502 Time consumed: 49.6542 s\n",
      "Epoch: 269 Train_loss: 1.312 Val_acc: 0.496 Time consumed: 49.5354 s\n",
      "Epoch: 270 Train_loss: 1.349 Val_acc: 0.500 Time consumed: 49.6151 s\n",
      "Epoch: 271 Train_loss: 1.386 Val_acc: 0.490 Time consumed: 49.7111 s\n",
      "Epoch: 272 Train_loss: 1.328 Val_acc: 0.485 Time consumed: 49.9816 s\n",
      "Epoch: 273 Train_loss: 1.388 Val_acc: 0.497 Time consumed: 50.0043 s\n",
      "Epoch: 274 Train_loss: 1.357 Val_acc: 0.492 Time consumed: 50.0680 s\n",
      "Epoch: 275 Train_loss: 1.365 Val_acc: 0.480 Time consumed: 49.7783 s\n",
      "Epoch: 276 Train_loss: 1.404 Val_acc: 0.478 Time consumed: 50.0155 s\n",
      "Epoch: 277 Train_loss: 1.348 Val_acc: 0.502 Time consumed: 49.9220 s\n",
      "Epoch: 278 Train_loss: 1.375 Val_acc: 0.496 Time consumed: 49.8340 s\n",
      "Epoch: 279 Train_loss: 1.328 Val_acc: 0.474 Time consumed: 50.2398 s\n",
      "Epoch: 280 Train_loss: 1.234 Val_acc: 0.477 Time consumed: 49.8907 s\n",
      "Epoch: 281 Train_loss: 1.304 Val_acc: 0.467 Time consumed: 50.1646 s\n",
      "Epoch: 282 Train_loss: 1.331 Val_acc: 0.500 Time consumed: 49.8554 s\n",
      "Epoch: 283 Train_loss: 1.419 Val_acc: 0.493 Time consumed: 49.7593 s\n",
      "Epoch: 284 Train_loss: 1.354 Val_acc: 0.501 Time consumed: 49.7764 s\n",
      "Epoch: 285 Train_loss: 1.366 Val_acc: 0.500 Time consumed: 50.0193 s\n",
      "Epoch: 286 Train_loss: 1.365 Val_acc: 0.472 Time consumed: 50.0142 s\n",
      "Epoch: 287 Train_loss: 1.414 Val_acc: 0.485 Time consumed: 50.2551 s\n",
      "Epoch: 288 Train_loss: 1.369 Val_acc: 0.498 Time consumed: 49.8934 s\n",
      "Epoch: 289 Train_loss: 1.333 Val_acc: 0.487 Time consumed: 50.3314 s\n",
      "Epoch: 290 Train_loss: 1.320 Val_acc: 0.482 Time consumed: 49.8859 s\n",
      "Epoch: 291 Train_loss: 1.383 Val_acc: 0.496 Time consumed: 49.6130 s\n",
      "Epoch: 292 Train_loss: 1.371 Val_acc: 0.497 Time consumed: 50.4057 s\n",
      "Epoch: 293 Train_loss: 1.338 Val_acc: 0.497 Time consumed: 50.0522 s\n",
      "Epoch: 294 Train_loss: 1.330 Val_acc: 0.494 Time consumed: 50.0357 s\n",
      "Epoch: 295 Train_loss: 1.353 Val_acc: 0.497 Time consumed: 50.0871 s\n",
      "Epoch: 296 Train_loss: 1.349 Val_acc: 0.491 Time consumed: 50.2293 s\n",
      "Epoch: 297 Train_loss: 1.337 Val_acc: 0.476 Time consumed: 50.4493 s\n",
      "Epoch: 298 Train_loss: 1.375 Val_acc: 0.501 Time consumed: 50.4769 s\n",
      "Epoch: 299 Train_loss: 1.330 Val_acc: 0.481 Time consumed: 50.0338 s\n",
      "Epoch: 300 Train_loss: 1.372 Val_acc: 0.494 Time consumed: 51.6714 s\n",
      "Epoch: 301 Train_loss: 1.363 Val_acc: 0.504 Time consumed: 49.6919 s\n",
      "Epoch: 302 Train_loss: 1.391 Val_acc: 0.502 Time consumed: 49.6900 s\n",
      "Epoch: 303 Train_loss: 1.344 Val_acc: 0.503 Time consumed: 49.5963 s\n",
      "Epoch: 304 Train_loss: 1.328 Val_acc: 0.501 Time consumed: 49.9293 s\n",
      "Epoch: 305 Train_loss: 1.311 Val_acc: 0.483 Time consumed: 49.9870 s\n",
      "Epoch: 306 Train_loss: 1.326 Val_acc: 0.492 Time consumed: 50.0664 s\n",
      "Epoch: 307 Train_loss: 1.219 Val_acc: 0.502 Time consumed: 50.2211 s\n",
      "Epoch: 308 Train_loss: 1.234 Val_acc: 0.490 Time consumed: 49.6398 s\n",
      "Epoch: 309 Train_loss: 1.311 Val_acc: 0.500 Time consumed: 50.1048 s\n",
      "Epoch: 310 Train_loss: 1.352 Val_acc: 0.493 Time consumed: 49.9898 s\n",
      "Epoch: 311 Train_loss: 1.337 Val_acc: 0.494 Time consumed: 50.0762 s\n",
      "Epoch: 312 Train_loss: 1.311 Val_acc: 0.492 Time consumed: 50.1527 s\n",
      "Epoch: 313 Train_loss: 1.330 Val_acc: 0.491 Time consumed: 50.1916 s\n",
      "Epoch: 314 Train_loss: 1.355 Val_acc: 0.508 Time consumed: 50.0832 s\n",
      "Epoch: 315 Train_loss: 1.279 Val_acc: 0.493 Time consumed: 49.9730 s\n",
      "Epoch: 316 Train_loss: 1.410 Val_acc: 0.483 Time consumed: 50.1599 s\n",
      "Epoch: 317 Train_loss: 1.301 Val_acc: 0.505 Time consumed: 49.8641 s\n",
      "Epoch: 318 Train_loss: 1.324 Val_acc: 0.494 Time consumed: 50.0508 s\n",
      "Epoch: 319 Train_loss: 1.363 Val_acc: 0.479 Time consumed: 49.7866 s\n",
      "Epoch: 320 Train_loss: 1.402 Val_acc: 0.501 Time consumed: 50.0246 s\n",
      "Epoch: 321 Train_loss: 1.312 Val_acc: 0.470 Time consumed: 50.1145 s\n",
      "Epoch: 322 Train_loss: 1.374 Val_acc: 0.482 Time consumed: 50.1168 s\n",
      "Epoch: 323 Train_loss: 1.291 Val_acc: 0.498 Time consumed: 50.2532 s\n",
      "Epoch: 324 Train_loss: 1.371 Val_acc: 0.495 Time consumed: 50.0086 s\n",
      "Epoch: 325 Train_loss: 1.418 Val_acc: 0.503 Time consumed: 50.1492 s\n",
      "Epoch: 326 Train_loss: 1.278 Val_acc: 0.515 Time consumed: 54.7092 s\n",
      "Epoch: 327 Train_loss: 1.274 Val_acc: 0.498 Time consumed: 49.9587 s\n",
      "Epoch: 328 Train_loss: 1.335 Val_acc: 0.503 Time consumed: 49.4556 s\n",
      "Epoch: 329 Train_loss: 1.393 Val_acc: 0.486 Time consumed: 49.8052 s\n",
      "Epoch: 330 Train_loss: 1.285 Val_acc: 0.499 Time consumed: 49.7869 s\n",
      "Epoch: 331 Train_loss: 1.329 Val_acc: 0.494 Time consumed: 49.8305 s\n",
      "Epoch: 332 Train_loss: 1.380 Val_acc: 0.497 Time consumed: 49.7777 s\n",
      "Epoch: 333 Train_loss: 1.280 Val_acc: 0.486 Time consumed: 50.0014 s\n",
      "Epoch: 334 Train_loss: 1.260 Val_acc: 0.504 Time consumed: 49.9823 s\n",
      "Epoch: 335 Train_loss: 1.294 Val_acc: 0.500 Time consumed: 49.9685 s\n",
      "Epoch: 336 Train_loss: 1.277 Val_acc: 0.494 Time consumed: 49.9796 s\n",
      "Epoch: 337 Train_loss: 1.324 Val_acc: 0.494 Time consumed: 49.9607 s\n",
      "Epoch: 338 Train_loss: 1.253 Val_acc: 0.500 Time consumed: 49.9191 s\n",
      "Epoch: 339 Train_loss: 1.290 Val_acc: 0.497 Time consumed: 49.7765 s\n",
      "Epoch: 340 Train_loss: 1.362 Val_acc: 0.500 Time consumed: 49.8195 s\n",
      "Epoch: 341 Train_loss: 1.329 Val_acc: 0.479 Time consumed: 50.2068 s\n",
      "Epoch: 342 Train_loss: 1.358 Val_acc: 0.487 Time consumed: 50.1706 s\n",
      "Epoch: 343 Train_loss: 1.357 Val_acc: 0.447 Time consumed: 49.9581 s\n",
      "Epoch: 344 Train_loss: 1.428 Val_acc: 0.496 Time consumed: 49.9588 s\n",
      "Epoch: 345 Train_loss: 1.279 Val_acc: 0.466 Time consumed: 49.9100 s\n",
      "Epoch: 346 Train_loss: 1.437 Val_acc: 0.490 Time consumed: 49.9062 s\n",
      "Epoch: 347 Train_loss: 1.412 Val_acc: 0.490 Time consumed: 50.1880 s\n",
      "Epoch: 348 Train_loss: 1.362 Val_acc: 0.489 Time consumed: 50.3499 s\n",
      "Epoch: 349 Train_loss: 1.354 Val_acc: 0.485 Time consumed: 50.0977 s\n",
      "Epoch: 350 Train_loss: 1.357 Val_acc: 0.496 Time consumed: 50.1446 s\n",
      "Epoch: 351 Train_loss: 1.239 Val_acc: 0.484 Time consumed: 50.2778 s\n",
      "Epoch: 352 Train_loss: 1.394 Val_acc: 0.494 Time consumed: 49.9071 s\n",
      "Epoch: 353 Train_loss: 1.325 Val_acc: 0.496 Time consumed: 49.9735 s\n",
      "Epoch: 354 Train_loss: 1.383 Val_acc: 0.492 Time consumed: 50.4399 s\n",
      "Epoch: 355 Train_loss: 1.363 Val_acc: 0.497 Time consumed: 50.3321 s\n",
      "Epoch: 356 Train_loss: 1.331 Val_acc: 0.507 Time consumed: 50.5071 s\n",
      "Epoch: 357 Train_loss: 1.380 Val_acc: 0.484 Time consumed: 50.2826 s\n",
      "Epoch: 358 Train_loss: 1.383 Val_acc: 0.502 Time consumed: 50.0703 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 359 Train_loss: 1.441 Val_acc: 0.500 Time consumed: 50.4857 s\n",
      "Epoch: 360 Train_loss: 1.262 Val_acc: 0.503 Time consumed: 51.6426 s\n",
      "Epoch: 361 Train_loss: 1.242 Val_acc: 0.500 Time consumed: 49.6378 s\n",
      "Epoch: 362 Train_loss: 1.256 Val_acc: 0.494 Time consumed: 49.8281 s\n",
      "Epoch: 363 Train_loss: 1.267 Val_acc: 0.487 Time consumed: 50.0152 s\n",
      "Epoch: 364 Train_loss: 1.247 Val_acc: 0.496 Time consumed: 49.7647 s\n",
      "Epoch: 365 Train_loss: 1.345 Val_acc: 0.487 Time consumed: 49.9101 s\n",
      "Epoch: 366 Train_loss: 1.276 Val_acc: 0.510 Time consumed: 49.7500 s\n",
      "Epoch: 367 Train_loss: 1.312 Val_acc: 0.500 Time consumed: 49.8721 s\n",
      "Epoch: 368 Train_loss: 1.334 Val_acc: 0.500 Time consumed: 50.0665 s\n",
      "Epoch: 369 Train_loss: 1.294 Val_acc: 0.496 Time consumed: 49.7992 s\n",
      "Epoch: 370 Train_loss: 1.324 Val_acc: 0.498 Time consumed: 49.9638 s\n",
      "Epoch: 371 Train_loss: 1.396 Val_acc: 0.497 Time consumed: 49.8231 s\n",
      "Epoch: 372 Train_loss: 1.341 Val_acc: 0.500 Time consumed: 50.2470 s\n",
      "Epoch: 373 Train_loss: 1.317 Val_acc: 0.500 Time consumed: 49.9743 s\n",
      "Epoch: 374 Train_loss: 1.370 Val_acc: 0.507 Time consumed: 49.9436 s\n",
      "Epoch: 375 Train_loss: 1.293 Val_acc: 0.507 Time consumed: 49.9283 s\n",
      "Epoch: 376 Train_loss: 1.255 Val_acc: 0.504 Time consumed: 49.9923 s\n",
      "Epoch: 377 Train_loss: 1.364 Val_acc: 0.503 Time consumed: 50.3452 s\n",
      "Epoch: 378 Train_loss: 1.324 Val_acc: 0.490 Time consumed: 50.1958 s\n",
      "Epoch: 379 Train_loss: 1.365 Val_acc: 0.497 Time consumed: 50.0989 s\n",
      "Epoch: 380 Train_loss: 1.263 Val_acc: 0.506 Time consumed: 49.9984 s\n",
      "Epoch: 381 Train_loss: 1.303 Val_acc: 0.491 Time consumed: 50.1645 s\n",
      "Epoch: 382 Train_loss: 1.276 Val_acc: 0.491 Time consumed: 50.2848 s\n",
      "Epoch: 383 Train_loss: 1.357 Val_acc: 0.504 Time consumed: 49.8318 s\n",
      "Epoch: 384 Train_loss: 1.320 Val_acc: 0.493 Time consumed: 50.0830 s\n",
      "Epoch: 385 Train_loss: 1.328 Val_acc: 0.481 Time consumed: 50.4618 s\n",
      "Epoch: 386 Train_loss: 1.373 Val_acc: 0.489 Time consumed: 50.2204 s\n",
      "Epoch: 387 Train_loss: 1.340 Val_acc: 0.496 Time consumed: 50.2022 s\n",
      "Epoch: 388 Train_loss: 1.324 Val_acc: 0.477 Time consumed: 50.4680 s\n",
      "Epoch: 389 Train_loss: 1.305 Val_acc: 0.426 Time consumed: 50.2558 s\n",
      "Epoch: 390 Train_loss: 1.368 Val_acc: 0.487 Time consumed: 50.3456 s\n",
      "Epoch: 391 Train_loss: 1.308 Val_acc: 0.509 Time consumed: 50.6394 s\n",
      "Epoch: 392 Train_loss: 1.313 Val_acc: 0.500 Time consumed: 50.0340 s\n",
      "Epoch: 393 Train_loss: 1.752 Val_acc: 0.481 Time consumed: 50.5396 s\n",
      "Epoch: 394 Train_loss: 1.262 Val_acc: 0.482 Time consumed: 51.6446 s\n",
      "Epoch: 395 Train_loss: 1.351 Val_acc: 0.489 Time consumed: 49.4742 s\n",
      "Epoch: 396 Train_loss: 1.402 Val_acc: 0.491 Time consumed: 49.7717 s\n",
      "Epoch: 397 Train_loss: 1.231 Val_acc: 0.489 Time consumed: 49.8841 s\n",
      "Epoch: 398 Train_loss: 1.313 Val_acc: 0.494 Time consumed: 49.9275 s\n",
      "Epoch: 399 Train_loss: 1.445 Val_acc: 0.495 Time consumed: 49.6117 s\n",
      "Epoch: 400 Train_loss: 1.341 Val_acc: 0.494 Time consumed: 50.0422 s\n",
      "Epoch: 401 Train_loss: 1.331 Val_acc: 0.481 Time consumed: 49.7975 s\n",
      "Epoch: 402 Train_loss: 1.372 Val_acc: 0.492 Time consumed: 49.8829 s\n",
      "Epoch: 403 Train_loss: 1.253 Val_acc: 0.490 Time consumed: 50.2988 s\n",
      "Epoch: 404 Train_loss: 1.229 Val_acc: 0.492 Time consumed: 49.7381 s\n",
      "Epoch: 405 Train_loss: 1.339 Val_acc: 0.502 Time consumed: 49.9767 s\n",
      "Epoch: 406 Train_loss: 1.396 Val_acc: 0.489 Time consumed: 50.2036 s\n",
      "Epoch: 407 Train_loss: 1.416 Val_acc: 0.488 Time consumed: 50.0862 s\n",
      "Epoch: 408 Train_loss: 1.284 Val_acc: 0.483 Time consumed: 49.8331 s\n",
      "Epoch: 409 Train_loss: 1.268 Val_acc: 0.490 Time consumed: 49.6250 s\n",
      "Epoch: 410 Train_loss: 1.283 Val_acc: 0.481 Time consumed: 50.0384 s\n",
      "Epoch: 411 Train_loss: 1.210 Val_acc: 0.492 Time consumed: 50.4393 s\n",
      "Epoch: 412 Train_loss: 1.278 Val_acc: 0.485 Time consumed: 49.7627 s\n",
      "Epoch: 413 Train_loss: 1.291 Val_acc: 0.489 Time consumed: 49.9258 s\n",
      "Epoch: 414 Train_loss: 1.292 Val_acc: 0.494 Time consumed: 50.2351 s\n",
      "Epoch: 415 Train_loss: 1.280 Val_acc: 0.502 Time consumed: 50.3820 s\n",
      "Epoch: 416 Train_loss: 1.314 Val_acc: 0.497 Time consumed: 50.1037 s\n",
      "Epoch: 417 Train_loss: 1.235 Val_acc: 0.479 Time consumed: 50.1956 s\n",
      "Epoch: 418 Train_loss: 1.390 Val_acc: 0.481 Time consumed: 50.2136 s\n",
      "Epoch: 419 Train_loss: 1.314 Val_acc: 0.478 Time consumed: 50.0865 s\n",
      "Epoch: 420 Train_loss: 1.299 Val_acc: 0.479 Time consumed: 50.6678 s\n",
      "Epoch: 421 Train_loss: 1.228 Val_acc: 0.501 Time consumed: 50.0852 s\n",
      "Epoch: 422 Train_loss: 1.237 Val_acc: 0.498 Time consumed: 50.1899 s\n",
      "Epoch: 423 Train_loss: 1.281 Val_acc: 0.494 Time consumed: 50.0860 s\n",
      "Epoch: 424 Train_loss: 1.276 Val_acc: 0.489 Time consumed: 50.2806 s\n",
      "Epoch: 425 Train_loss: 1.378 Val_acc: 0.495 Time consumed: 50.3250 s\n",
      "Epoch: 426 Train_loss: 1.316 Val_acc: 0.494 Time consumed: 50.4814 s\n",
      "Epoch: 427 Train_loss: 1.268 Val_acc: 0.492 Time consumed: 50.3796 s\n",
      "Epoch: 428 Train_loss: 1.256 Val_acc: 0.499 Time consumed: 51.6343 s\n",
      "Epoch: 429 Train_loss: 1.344 Val_acc: 0.489 Time consumed: 50.0162 s\n",
      "Epoch: 430 Train_loss: 1.194 Val_acc: 0.486 Time consumed: 49.7463 s\n",
      "Epoch: 431 Train_loss: 1.247 Val_acc: 0.509 Time consumed: 49.8393 s\n",
      "Epoch: 432 Train_loss: 1.230 Val_acc: 0.491 Time consumed: 49.8228 s\n",
      "Epoch: 433 Train_loss: 1.272 Val_acc: 0.497 Time consumed: 50.0827 s\n",
      "Epoch: 434 Train_loss: 1.278 Val_acc: 0.481 Time consumed: 49.9265 s\n",
      "Epoch: 435 Train_loss: 1.315 Val_acc: 0.491 Time consumed: 49.9851 s\n",
      "Epoch: 436 Train_loss: 1.299 Val_acc: 0.495 Time consumed: 50.1818 s\n",
      "Epoch: 437 Train_loss: 1.243 Val_acc: 0.504 Time consumed: 50.4299 s\n",
      "Epoch: 438 Train_loss: 1.384 Val_acc: 0.491 Time consumed: 49.7763 s\n",
      "Epoch: 439 Train_loss: 1.231 Val_acc: 0.499 Time consumed: 50.4876 s\n",
      "Epoch: 440 Train_loss: 1.286 Val_acc: 0.489 Time consumed: 50.2171 s\n",
      "Epoch: 441 Train_loss: 1.209 Val_acc: 0.487 Time consumed: 50.1057 s\n",
      "Epoch: 442 Train_loss: 1.233 Val_acc: 0.506 Time consumed: 49.7806 s\n",
      "Epoch: 443 Train_loss: 1.391 Val_acc: 0.489 Time consumed: 50.0370 s\n",
      "Epoch: 444 Train_loss: 1.225 Val_acc: 0.502 Time consumed: 49.8073 s\n",
      "Epoch: 445 Train_loss: 1.359 Val_acc: 0.489 Time consumed: 50.0184 s\n",
      "Epoch: 446 Train_loss: 1.184 Val_acc: 0.502 Time consumed: 50.2228 s\n",
      "Epoch: 447 Train_loss: 1.344 Val_acc: 0.502 Time consumed: 50.1860 s\n",
      "Epoch: 448 Train_loss: 1.423 Val_acc: 0.501 Time consumed: 50.0179 s\n",
      "Epoch: 449 Train_loss: 1.363 Val_acc: 0.490 Time consumed: 50.1207 s\n",
      "Epoch: 450 Train_loss: 1.201 Val_acc: 0.507 Time consumed: 50.2115 s\n",
      "Epoch: 451 Train_loss: 1.261 Val_acc: 0.489 Time consumed: 50.3004 s\n",
      "Epoch: 452 Train_loss: 1.212 Val_acc: 0.508 Time consumed: 50.3356 s\n",
      "Epoch: 453 Train_loss: 1.251 Val_acc: 0.491 Time consumed: 50.2659 s\n",
      "Epoch: 454 Train_loss: 1.359 Val_acc: 0.485 Time consumed: 50.6744 s\n",
      "Epoch: 455 Train_loss: 1.244 Val_acc: 0.501 Time consumed: 52.4180 s\n",
      "Epoch: 456 Train_loss: 1.421 Val_acc: 0.495 Time consumed: 53.2323 s\n",
      "Epoch: 457 Train_loss: 1.161 Val_acc: 0.492 Time consumed: 53.1996 s\n",
      "Epoch: 458 Train_loss: 1.332 Val_acc: 0.492 Time consumed: 53.2102 s\n",
      "Epoch: 459 Train_loss: 1.283 Val_acc: 0.491 Time consumed: 52.1406 s\n",
      "Epoch: 460 Train_loss: 1.234 Val_acc: 0.499 Time consumed: 51.8493 s\n",
      "Epoch: 461 Train_loss: 1.286 Val_acc: 0.489 Time consumed: 52.3215 s\n",
      "Epoch: 462 Train_loss: 1.334 Val_acc: 0.494 Time consumed: 52.8053 s\n",
      "Epoch: 463 Train_loss: 1.307 Val_acc: 0.497 Time consumed: 50.8094 s\n",
      "Epoch: 464 Train_loss: 1.345 Val_acc: 0.501 Time consumed: 49.8566 s\n",
      "Epoch: 465 Train_loss: 1.223 Val_acc: 0.498 Time consumed: 49.9782 s\n",
      "Epoch: 466 Train_loss: 1.288 Val_acc: 0.494 Time consumed: 49.6897 s\n",
      "Epoch: 467 Train_loss: 1.325 Val_acc: 0.507 Time consumed: 49.7691 s\n",
      "Epoch: 468 Train_loss: 1.389 Val_acc: 0.495 Time consumed: 49.8810 s\n",
      "Epoch: 469 Train_loss: 1.318 Val_acc: 0.492 Time consumed: 50.0732 s\n",
      "Epoch: 470 Train_loss: 1.413 Val_acc: 0.494 Time consumed: 50.1338 s\n",
      "Epoch: 471 Train_loss: 1.352 Val_acc: 0.499 Time consumed: 49.9482 s\n",
      "Epoch: 472 Train_loss: 1.337 Val_acc: 0.502 Time consumed: 49.9049 s\n",
      "Epoch: 473 Train_loss: 1.276 Val_acc: 0.499 Time consumed: 49.6722 s\n",
      "Epoch: 474 Train_loss: 1.246 Val_acc: 0.506 Time consumed: 49.8793 s\n",
      "Epoch: 475 Train_loss: 1.250 Val_acc: 0.496 Time consumed: 50.0372 s\n",
      "Epoch: 476 Train_loss: 1.222 Val_acc: 0.492 Time consumed: 50.0260 s\n",
      "Epoch: 477 Train_loss: 1.295 Val_acc: 0.501 Time consumed: 50.3202 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 478 Train_loss: 1.278 Val_acc: 0.503 Time consumed: 50.2957 s\n",
      "Epoch: 479 Train_loss: 1.307 Val_acc: 0.488 Time consumed: 50.1434 s\n",
      "Epoch: 480 Train_loss: 1.312 Val_acc: 0.499 Time consumed: 50.2255 s\n",
      "Epoch: 481 Train_loss: 1.381 Val_acc: 0.489 Time consumed: 50.2426 s\n",
      "Epoch: 482 Train_loss: 1.300 Val_acc: 0.495 Time consumed: 50.0194 s\n",
      "Epoch: 483 Train_loss: 1.222 Val_acc: 0.511 Time consumed: 50.2722 s\n",
      "Epoch: 484 Train_loss: 1.237 Val_acc: 0.511 Time consumed: 50.6186 s\n",
      "Epoch: 485 Train_loss: 1.205 Val_acc: 0.496 Time consumed: 50.2295 s\n",
      "Epoch: 486 Train_loss: 1.348 Val_acc: 0.514 Time consumed: 50.0404 s\n",
      "Epoch: 487 Train_loss: 1.273 Val_acc: 0.507 Time consumed: 50.5372 s\n",
      "Epoch: 488 Train_loss: 1.300 Val_acc: 0.510 Time consumed: 50.3514 s\n",
      "Epoch: 489 Train_loss: 1.271 Val_acc: 0.501 Time consumed: 50.3793 s\n",
      "Epoch: 490 Train_loss: 1.250 Val_acc: 0.495 Time consumed: 51.8231 s\n",
      "Epoch: 491 Train_loss: 1.233 Val_acc: 0.511 Time consumed: 52.4650 s\n",
      "Epoch: 492 Train_loss: 1.171 Val_acc: 0.506 Time consumed: 51.0587 s\n",
      "Epoch: 493 Train_loss: 1.183 Val_acc: 0.503 Time consumed: 50.1410 s\n",
      "Epoch: 494 Train_loss: 1.254 Val_acc: 0.510 Time consumed: 51.9140 s\n",
      "Epoch: 495 Train_loss: 1.276 Val_acc: 0.496 Time consumed: 52.0560 s\n",
      "Epoch: 496 Train_loss: 1.318 Val_acc: 0.485 Time consumed: 53.2603 s\n",
      "Epoch: 497 Train_loss: 1.302 Val_acc: 0.504 Time consumed: 52.0620 s\n",
      "Epoch: 498 Train_loss: 1.291 Val_acc: 0.491 Time consumed: 50.8096 s\n",
      "Epoch: 499 Train_loss: 1.158 Val_acc: 0.511 Time consumed: 49.8402 s\n",
      "Epoch: 500 Train_loss: 1.270 Val_acc: 0.501 Time consumed: 49.7512 s\n",
      "*****************Training End!*****************\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "    print('*****************Training Start!*****************')\n",
    "    train_writer = tf.summary.FileWriter(path_logdir+'train', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for m in range(epochs):\n",
    "        start = time.time()\n",
    "        batch_gen = datagen.flow(x_train, y_train, batch_size=batch_size*num_gpu)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            x_batch, y_batch = next(batch_gen)\n",
    "            _, loss_train, summary = sess.run([train_op, losses, merged],\n",
    "                                              {inputs: x_batch, outputs: y_batch, is_train: True})\n",
    "            train_writer.add_summary(summary, m * iterations + i)\n",
    "        \n",
    "        val_accs = []\n",
    "        for i in range(5000//(batch_size*num_gpu)):\n",
    "            val_acc = sess.run(accs,{inputs: x_test[i*batch_size*num_gpu:(i+1)*num_gpu*batch_size], \n",
    "                                     outputs: y_test[i*batch_size*num_gpu:(i+1)*num_gpu*batch_size], \n",
    "                                     is_train: False})\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        if np.mean(val_accs) > old_acc:\n",
    "            old_acc = np.mean(val_accs)\n",
    "            saver.save(sess, path_model+'cifar10.ckpt')\n",
    "\n",
    "        end = time.time()\n",
    "        print('Epoch: {}'.format(m + 1),\n",
    "              'Train_loss: {:.3f}'.format(loss_train),\n",
    "              'Val_acc: {:.3f}'.format(np.mean(val_accs)),\n",
    "              'Time consumed: {:.4f} s'.format(end - start))\n",
    "\n",
    "    print('*****************Training End!*****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
